{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"jANemgV_FkJP","executionInfo":{"status":"ok","timestamp":1710778825933,"user_tz":180,"elapsed":333,"user":{"displayName":"Mayron Robert","userId":"07786555594977039272"}}},"outputs":[],"source":["#Libriries\n","import numpy as np\n","import tensorflow as tf\n","from cross_entropy_study import *"]},{"cell_type":"code","source":["print(\"########### Example 1 ###########\")\n","# Compare with Keras' CategoricalCrossentropy\n","# Note that TF/Keras calls the unnormalized input to softmax\n","# as \"logits\", but they are simply inputs to softmax. Strictly,\n","# logits are related to the sigmoid function, not softmax.\n","# See reference [3] for a discussion about the confusing TF/Keras nomenclature.\n","true_pmf = np.array([0, 1, 0, 0])\n","logits = np.array([-18.6, 0.51, 2.94, -12.8]) #strictly, these are not \"logits\"\n","unnormalized_probabilities = np.exp(logits)\n","estimated_pmf = unnormalized_probabilities/np.sum(unnormalized_probabilities) #softmax\n","print(\"estimated_pmf\", estimated_pmf, np.sum(estimated_pmf))\n","\n","cce = tf.keras.losses.CategoricalCrossentropy()\n","print(\"Cross-entropy (BCE) calculated by Keras =\",cce(true_pmf, estimated_pmf).numpy())\n","\n","cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","print(\"Cross-entropy (BCE) from logits calculated by Keras =\",cce(true_pmf, logits).numpy())\n","\n","cross_entropy_example = cross_entropy(true_pmf, estimated_pmf, use_log2 = False)\n","print('Our cross_entropy implementation=',cross_entropy_example)\n","\n"],"metadata":{"id":"9SGV6n8X3cU-","executionInfo":{"status":"ok","timestamp":1710778826581,"user_tz":180,"elapsed":3,"user":{"displayName":"Mayron Robert","userId":"07786555594977039272"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02a79d12-3cfb-41bb-afb5-1781c1e7b2aa"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["########### Example 1 ###########\n","estimated_pmf [4.06118903e-10 8.09134563e-02 9.19086409e-01 1.34140895e-07] 1.0\n","Cross-entropy (BCE) calculated by Keras = 2.514375135884203\n","Cross-entropy (BCE) from logits calculated by Keras = 2.514375135884203\n","Our cross_entropy implementation= 2.514375135884203\n"]}]},{"cell_type":"code","source":["print(\"########### Example 2 ###########\")\n","# Compare with Keras' BinaryCrossentropy, which is used when one has\n","# arrays with N probabilities, from N binomial distributions.\n","# The binary cross-entropy is called BCE.\n","# In this case, the unnormalized inputs are effectively the logits.\n","# Note that we use the sigmoid, not the softmax below.\n","true_pmf = np.array([0, 1, 1, 0])\n","logits = np.array([-18.6, 0.51, 2.94, -12.8])\n","probabilities_via_sigmoid = 1.0 / (1 + np.exp(-logits) ) #logit relation to sigmoid\n","estimated_pmf = probabilities_via_sigmoid\n","# Here there are N binomial distributions, and they do not sum up to one, because\n","# they are N different distributions.\n","print(\"Estimated probability for binary PMFs (note that collectively, they do not sum up to 1\", estimated_pmf, np.sum(estimated_pmf))\n","\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","print(\"binary cross-entropy (BCE) from logits by Keras =\",bce(true_pmf, logits).numpy())\n","\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","print(\"binary cross-entropy (BCE) by Keras =\",bce(true_pmf, estimated_pmf).numpy())\n","\n","bce2 = binary_cross_entropy(true_pmf, estimated_pmf, use_log2 = False)\n","print(\"binary cross-entropy (BCE) =\", bce2)\n","\n"],"metadata":{"id":"Ghd3CJ_u3f22","executionInfo":{"status":"ok","timestamp":1710778826581,"user_tz":180,"elapsed":2,"user":{"displayName":"Mayron Robert","userId":"07786555594977039272"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fd89ef4-357d-49ec-e065-cdd1ef19cc1f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["########### Example 2 ###########\n","Estimated probability for binary PMFs (note that collectively, they do not sum up to 1 [8.35839003e-09 6.24806474e-01 9.49788727e-01 2.76076495e-06] 1.5745979704015032\n","binary cross-entropy (BCE) from logits by Keras = 0.13045794978109726\n","binary cross-entropy (BCE) by Keras = 0.1304578563574039\n","binary cross-entropy (BCE) = 0.1304579497810972\n"]}]},{"cell_type":"code","source":["print(\"########### Example 3 ###########\")\n","# Now use another function to calculate the BCE. This functions deals\n","# with a single pair of binomial distributions, represented by probabilities\n","# p and q.\n","p=0.7\n","q=0.3\n","\n","# First option is to create the pair of binomial distributions and call\n","# the general method, that calculates the average cross-entropy between\n","# these two distributions\n","true_pmf = np.array([p,1-p]) #first binomial distribution\n","estimated_pmf = np.array([q,1-q]) #second binomial distribution\n","#compare with Keras:\n","bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","print(\"binary cross-entropy (BCE) by Keras =\",bce(true_pmf, estimated_pmf).numpy())\n","\n","bce2 = binary_cross_entropy(true_pmf, estimated_pmf, use_log2 = False)\n","print(\"binary cross-entropy (BCE) =\", bce2)\n","\n","# Function that deals with a single pair of binomial distributions,\n","# represented by probabilities p and q\n","bce3 = individual_binary_cross_entropy_v2(p, q, use_log2 = False)\n","print(\"binary cross-entropy (BCE) =\", bce3)"],"metadata":{"id":"p6QFL8Rr3h1V","executionInfo":{"status":"ok","timestamp":1710778826581,"user_tz":180,"elapsed":2,"user":{"displayName":"Mayron Robert","userId":"07786555594977039272"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2730427e-cd48-4ce6-875b-ef1726152f74"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["########### Example 3 ###########\n","binary cross-entropy (BCE) by Keras = 0.9497831700193407\n","binary cross-entropy (BCE) = 0.949783446209775\n","binary cross-entropy (BCE) = 0.949783446209775\n"]}]}]}